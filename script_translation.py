# -*- coding: utf-8 -*-
"""Hate Speech - TextTraslationToMg.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vzoa-oZphVH-qMU0D1LmP6OQreaP76f_

### Script translation text(en) to malagasy(mg)
"""

import subprocess
import sys

def install_package(package_name):
    try:
        subprocess.check_call([sys.executable, "-m", "pip", "install", "-U", package_name])
        print(f"Package '{package_name}' installé ou mis à jour avec succès.")
        return True
    except subprocess.CalledProcessError as e:
        print(f"Erreur lors de l'installation du package '{package_name}': {e}")
        return False

install_package('tqdm')
install_package('pandas')
install_package("bitsandbytes")
install_package("kaggle")
install_package('transformers')
install_package('torch')
install_package('wandb')
install_package('kagglehub')

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import pandas as pd
import torch
from tqdm.auto import tqdm
import wandb
import argparse
import kagglehub
from kagglehub import KaggleDatasetAdapter

# Définir les arguments pour le script
parser = argparse.ArgumentParser(description='Translate a portion of a CSV file using MADLAD-400-7B-MT-BT.')
parser.add_argument('--start_index', type=int, default=0, help='Index de la première ligne à traiter (inclusive).')
parser.add_argument('--end_index', type=int, default=10000, help='Index de la dernière ligne à traiter (exclusive).')
parser.add_argument('--batch_size', type=int, default=16, help='Taille du lot pour la traduction.')
args = parser.parse_args()

start_index = args.start_index
end_index = args.end_index
batch_size = args.batch_size

# Vérifier si un GPU est disponible et le définir comme device, sinon utiliser le CPU
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Utilisation de l'appareil : {device}")

# modèle MADLAD-400-7B-MT-BT avec quantification 8 bits
model_name = "google/madlad400-7b-mt-bt"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name, load_in_8bit=True, device_map='auto')

# file_path = 'https://drive.google.com/file/d/1WnF1ZBRCULI2vVwnJU_a1X0z1_jZRJij/view?usp=sharing'
try:
    # The following code will only execute
    # successfully when compression is complete

    # Install dependencies as needed:
    # pip install kagglehub[pandas-datasets]

    # Set the path to the file you'd like to load
    file_path = "data_combined (1).csv"

    # Load the latest version
    df = kagglehub.load_dataset(
      KaggleDatasetAdapter.PANDAS,
      "fordev0/hate-vazah",
      file_path,
      # Provide any additional arguments like 
      # sql_query or pandas_kwargs. See the 
      # documenation for more information:
      # https://github.com/Kaggle/kagglehub/blob/main/README.md#kaggledatasetadapterpandas
    )
except FileNotFoundError:
    print(f"Erreur: Le fichier '{file_path}' n'a pas été trouvé.")
    exit()

# Vérifier que la colonne 'text' existe
if 'text' not in df.columns:
    print("Erreur: La colonne 'text' n'existe pas dans le fichier CSV.")
    exit()

# Sélectionner la portion du DataFrame à traiter
df_subset = df[start_index:end_index]
print(f"Traitement des lignes de {start_index} à {end_index - 1} par lots de {batch_size}.")

def translate_batch(texts):
    input_texts = [f"<2mg> {text}" for text in texts]  # Ajouter le préfixe de langue: 2mg
    inputs = tokenizer(input_texts, return_tensors="pt", padding=True, truncation=True).to(device)
    with torch.no_grad():
        generated_tokens = model.generate(
            **inputs,
            max_length=512
        )
    translated_texts = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)
    return translated_texts

translations = [''] * len(df_subset)  # Pré-allouer la liste des traductions

# Appliquer la traduction par lots avec une barre de progression
for i in tqdm(range(0, len(df_subset), batch_size), desc="Traduction par lots"):
    batch = df_subset['text'][i:i + batch_size].tolist()
    translated_batch = translate_batch(batch)
    translations[i:i + batch_size] = translated_batch

# Ajouter la liste des traductions comme une nouvelle colonne au sous-ensemble
df_subset['translated'] = translations

# Initialiser WandB
wandb.init(project="hate-speech-translated-malagasy")
# Créer un tableau WandB à partir du sous-ensemble traduit (contenant 'text', 'label' et 'translated')
wandb_table = wandb.Table(data=df_subset[['text', 'label', 'translated']], columns=["text", "label", "translated"])

# Enregistrer le tableau dans WandB
wandb.log({"translations": wandb_table})

# Fermer la run WandB
wandb.finish()

