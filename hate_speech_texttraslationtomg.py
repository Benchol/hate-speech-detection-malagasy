# -*- coding: utf-8 -*-
"""Hate Speech - TextTraslationToMg.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vzoa-oZphVH-qMU0D1LmP6OQreaP76f_

### Script translation text(en) to malagasy(mg)
"""

import subprocess
import sys

def install_and_import(package):
    try:
        __import__(package)
    except ImportError:
        print(f"Installation de {package}...")
        subprocess.check_call([sys.executable, "-m", "pip", "install", package])
    finally:
        globals()[package] = __import__(package)

install_and_import('transformers')
install_and_import('pandas')
install_and_import('torch')
install_and_import('google.colab')
install_and_import('tqdm.auto')
install_and_import('wandb')
install_and_import('argparse')
install_and_import('bitsandbytes')

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import pandas as pd
import torch
from google.colab import drive
from tqdm.auto import tqdm
import wandb
import argparse
import bitsandbytes

from google.colab import drive

drive.mount("/content/drive")

# Définir les arguments pour le script
parser = argparse.ArgumentParser(description='Translate a portion of a CSV file using MADLAD-400-7B-MT-BT.')
parser.add_argument('--start_index', type=int, default=0, help='Index de la première ligne à traiter (inclusive).')
parser.add_argument('--end_index', type=int, default=10000, help='Index de la dernière ligne à traiter (exclusive).')
parser.add_argument('--batch_size', type=int, default=16, help='Taille du lot pour la traduction.')
args = parser.parse_args()

start_index = args.start_index
end_index = args.end_index
batch_size = args.batch_size

# Vérifier si un GPU est disponible et le définir comme device, sinon utiliser le CPU
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Utilisation de l'appareil : {device}")

# modèle MADLAD-400-7B-MT-BT
model_name = "google/madlad400-7b-mt-bt"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name, load_in_8bit=True, device_map='auto').to(device)

# Monter Google Drive pour le chargement et les sauvegardes des données
drive.mount('/content/drive')

file_path = '/content/drive/MyDrive/hate_speech/data_combined.csv'
try:
    df = pd.read_csv(file_path)
except FileNotFoundError:
    print(f"Erreur: Le fichier '{file_path}' n'a pas été trouvé.")
    exit()

# Vérifier que la colonne 'text' existe
if 'text' not in df.columns:
    print("Erreur: La colonne 'text' n'existe pas dans le fichier CSV.")
    exit()

# Sélectionner la portion du DataFrame à traiter
df_subset = df[start_index:end_index]
print(f"Traitement des lignes de {start_index} à {end_index - 1} par lots de {batch_size}.")

def translate_batch(texts):
    input_texts = [f"<2mg> {text}" for text in texts]  # Ajouter le préfixe de langue
    inputs = tokenizer(input_texts, return_tensors="pt", padding=True, truncation=True).to(device)
    with torch.no_grad():
        generated_tokens = model.generate(
            **inputs,
            max_length=512
        )
    translated_texts = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)
    return translated_texts

translations = [''] * len(df_subset)  # Pré-allouer la liste des traductions

# Appliquer la traduction par lots avec une barre de progression
for i in tqdm(range(0, len(df_subset), batch_size), desc="Traduction par lots"):
    batch = df_subset['text'][i:i + batch_size].tolist()
    translated_batch = translate_batch(batch)
    translations[i:i + batch_size] = translated_batch

# Ajouter la liste des traductions comme une nouvelle colonne au sous-ensemble
df_subset['translated'] = translations

# Initialiser WandB
wandb.init(project="hate-speech-translated-malagasy")
# Créer un tableau WandB à partir du sous-ensemble traduit (contenant 'text', 'label' et 'translated')
wandb_table = wandb.Table(data=df_subset[['text', 'label', 'translated']], columns=["text", "label", "translated"])

# Enregistrer le tableau dans WandB
wandb.log({"translations": wandb_table})

# Fermer la run WandB
wandb.finish()

