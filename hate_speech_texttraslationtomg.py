# -*- coding: utf-8 -*-
"""Hate Speech - TextTraslationToMg.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vzoa-oZphVH-qMU0D1LmP6OQreaP76f_

### Script translation text(en) to malagasy(mg)
"""

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import pandas as pd
import torch
from google.colab import drive
from tqdm.auto import tqdm
import wandb

from google.colab import drive

drive.mount("/content/drive")

# Définir les index de début et de fin pour cette exécution
start_index = 0
end_index = 10000  # Exclut cette ligne
batch_size = 16  # Taille du lot pour la traduction

# Vérifier si un GPU est disponible et le définir comme device, sinon utiliser le CPU
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Utilisation de l'appareil : {device}")

# modèle MADLAD-400-7B-MT-BT
model_name = "google/madlad400-7b-mt-bt"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)

# Monter Google Drive pour le chargement et les sauvegardes des données
drive.mount('/content/drive')

file_path = '/content/drive/MyDrive/hate_speech/data_combined.csv'
try:
    df = pd.read_csv(file_path)
except FileNotFoundError:
    print(f"Erreur: Le fichier '{file_path}' n'a pas été trouvé.")
    exit()

# Vérifier que la colonne 'text' existe
if 'text' not in df.columns:
    print("Erreur: La colonne 'text' n'existe pas dans le fichier CSV.")
    exit()

# Sélectionner la portion du DataFrame à traiter
df_subset = df[start_index:end_index]
print(f"Traitement des lignes de {start_index} à {end_index - 1} par lots de {batch_size}.")

def translate_batch(texts):
    input_texts = [f"<2mg> {text}" for text in texts]  # Ajouter le préfixe de langue
    inputs = tokenizer(input_texts, return_tensors="pt", padding=True, truncation=True).to(device)
    with torch.no_grad():
        generated_tokens = model.generate(
            **inputs,
            max_length=512
        )
    translated_texts = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)
    return translated_texts

translations = [''] * len(df_subset)  # Pré-allouer la liste des traductions

# Appliquer la traduction par lots avec une barre de progression
for i in tqdm(range(0, len(df_subset), batch_size), desc="Traduction par lots"):
    batch = df_subset['text'][i:i + batch_size].tolist()
    translated_batch = translate_batch(batch)
    translations[i:i + batch_size] = translated_batch

# Ajouter la liste des traductions comme une nouvelle colonne au sous-ensemble
df_subset['translated'] = translations

# Initialiser WandB
wandb.init(project="hate-speech-translated-malagasy")
# Créer un tableau WandB à partir du sous-ensemble traduit (contenant 'text', 'label' et 'translated')
wandb_table = wandb.Table(data=df_subset[['text', 'label', 'translated']], columns=["text", "label", "translated"])

# Enregistrer le tableau dans WandB
wandb.log({"translations": wandb_table})

# Fermer la run WandB
wandb.finish()

