# -*- coding: utf-8 -*-
"""XGBoost tfidf(n-grams) | Naive Bayes MultiNomialNB

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AklyCiVdxFs0JWwOH4RqsbN3-TsBAP8w
"""

"""### Import necessary"""

# === Import necessary libraries ===
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, ConfusionMatrixDisplay, roc_curve, auc
from xgboost import XGBClassifier
from imblearn.over_sampling import SMOTE
import matplotlib.pyplot as plt
import json
import re
import numpy as np

# Check for GPU availability
import torch
device = "cuda" if torch.cuda.is_available() else "cpu"

"""### import and visualize data"""

from google.colab import drive
drive.mount('/content/drive')

# Load and select columns
dataFrame = pd.read_csv("/content/drive/MyDrive/hate_speech/cb_data_mano.csv")
dataFrame = dataFrame[['text', 'label']]

dataFrame.head(5)

display(dataFrame.info())
# === Drop rows with missing values ===
dataFrame = dataFrame.dropna()

# === Text cleaning ===
mots_vides = ["voir plus", "lire la suite", "en savoir plus"]

def nettoyer_texte(texte):
    texte = texte.lower()
    for mot in mots_vides:
        texte = texte.replace(mot, "")

    # Remove URLs
    texte = re.sub(r'http\S+|www\S+|https\S+', '', texte)

    # Remove numbers
    texte = re.sub(r'\d+', '', texte)

    return texte.strip()

# Clean the text data
dataFrame["text"] = dataFrame["text"].apply(nettoyer_texte)

# Ensure labels are integers
dataFrame['label'] = dataFrame['label'].astype(float).astype('int')

display(dataFrame.info())

# === Visualize the distribution of labels ===
# 0 -> non-hate
# 1 -> hate
colors = ['#1f77b4', '#d62728']

label_counts = dataFrame['label'].value_counts()
total = label_counts.sum()

plt.figure(figsize=(8, 6))
bars = plt.bar(label_counts.index, label_counts.values, color=colors, alpha=0.85)
plt.title("R√©partition des √©tiquettes", fontsize=16)
plt.xlabel("Classe", fontsize=12)
plt.ylabel("Nombre d'occurrences", fontsize=12)
plt.xticks([0, 1], labels=["Non-haineux (0)", "Haineux (1)"], fontsize=11)

for i, bar in enumerate(bars):
    count = int(bar.get_height())
    percent = count / total * 100
    plt.text(bar.get_x() + bar.get_width() / 2, count + 1,
             f"{count} ({percent:.1f}%)",
             ha='center', va='bottom', fontsize=11)

plt.show()

"""*   On peut voir par ce graphe que les donn√©es sont mal proportion√©s
*   Pour une prediction avec plus de precision, il nous faut √©quilibrer ces don√©es. Et pour ce faire on va faire une "Data augmentation" avec des mots cl√©s qui favorisent la classe "Haine"

source: https://github.com/laravelmalagasy
"""

with open('/content/list.json', 'r', encoding='utf-8') as file:
    data = json.load(file)

data

"""#### Pre-processing: Increase the weighting of hate-indicating words"""

def augment_text_with_keywords(text, keywords, multiplier=3):
    words = text.split()
    augmented_text = text + " " + " ".join([word for word in words if word in keywords] * (multiplier - 1))
    return augmented_text

# Initialise list to store hate keywords
hate_keywords = []

# Loop through the data to extract sentences and variants
for item in data:
    # Extract the sentence and variants
    sentence = item["sentences"]
    variants = item["variants"]

    hate_keywords.append(sentence)
    for i in variants:
        hate_keywords.append(i)

# show the hate keywords
print(hate_keywords)

# apply the augmentation function to the 'text' column
dataFrame['text'] = dataFrame['text'].apply(lambda x: augment_text_with_keywords(x, hate_keywords))

"""### Prepare the data for training and testing"""

X = dataFrame["text"]
y = dataFrame["label"]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Vectorisation avec TF-IDF
tfidf = TfidfVectorizer(ngram_range=(1, 4), max_features=10000)  # Using n-grams from 1 to 4 and limiting to 10,000 features
X_train_tfidf = tfidf.fit_transform(X_train)
X_test_tfidf = tfidf.transform(X_test)

# Define the XGBoost model with class imbalance handling
model = XGBClassifier(scale_pos_weight=len(y_train[y_train == 0]) / len(y_train[y_train == 1]),
                      use_label_encoder=False, eval_metric="logloss", random_state=42)

# Training the model
model.fit(X_train_tfidf, y_train)

"""#### Pr√©dictions et √©valuation"""

y_pred = model.predict(X_test_tfidf)

# Results
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

y_proba = model.predict_proba(X_test_tfidf)[:, 1]  # Probabitility for the positive class

# Confusion matrix
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap="Blues")
plt.title("Matrice de confusion")
plt.show()

# Courbe ROC (binary)
fpr, tpr, thresholds = roc_curve(y_test, y_proba)
roc_auc = auc(fpr, tpr)

plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--') 
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('Taux de faux positifs (FPR)')
plt.ylabel('Taux de vrais positifs (TPR)')
plt.title('Courbe ROC')
plt.legend(loc="lower right")
plt.grid(True)
plt.show()



# Test function to classify a phrase using the trained model and TF-IDF vectorizer
def test_phrase(model, tfidf, phrase):
    # Transformer la phrase en vecteur TF-IDF
    phrase_tfidf = tfidf.transform([phrase])

    # Pr√©dire la classe (0 = Non-Hate Speech, 1 = Hate Speech)
    prediction = model.predict(phrase_tfidf)

    # Retourner la classe pr√©dite
    return "Hate Speech" if prediction[0] == 1 else "Non-Hate Speech"

# Exemple de test
example_1 = "Noana ah zany"
print(f"The phrase '{example_1}' is classified as: {test_phrase(model, tfidf, example_1)}")

example_2 = "Ry vahoaka malagasy, tiako ny tanindrazako"
print(f"The phrase '{example_2}' is classified as: {test_phrase(model, tfidf, example_2)}")

example_3 = "Matesa daholo, indrindra ilay mpialona ireny"
print(f"The phrase '{example_3}' is classified as: {test_phrase(model, tfidf, example_3)}")

example_4 = "Ratsy be pory tenisany amin'ireny akanjo mavomavo be ireny."
print(f"The phrase '{example_4}' is classified as: {test_phrase(model, tfidf, example_4)}")

example_5 = "Otran'ny alika nareo"
print(f"The phrase '{example_5}' is classified as: {test_phrase(model, tfidf, example_5)}")

example_6 = "kindindrenin'ity zany dia manahirana olona foana"
print(f"The phrase '{example_6}' is classified as: {test_phrase(model, tfidf, example_6)}")






"""-

-

-

## Naive base MultiNomial

Pour avoir une meilleur performance de ce model, on va faire appliquer la technique pour r√©soudre le probl√®me de d√©s√©quilibre des classes dans les ensembles de donn√©es.

SMOTE g√©n√®re des exemples synth√©tiques pour la classe minoritaire en interpolant les donn√©es existantes. Contrairement √† un simple sur-√©chantillonnage (qui duplique les donn√©es existantes), SMOTE cr√©e de nouvelles instances en :

Choisissant un exemple de la classe minoritaire au hasard.
Trouvant ses
ùëò
k-plus proches voisins (par d√©faut,
ùëò
=
5
k=5).
Cr√©ant un nouvel exemple en interpolant entre l'exemple choisi et l'un de ses voisins.
"""

# Smote for increasing the minority class weighting
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train_tfidf, y_train)

# Entra√Æner un mod√®le Multinomial Naive Bayes
model = MultinomialNB()
model.fit(X_train_tfidf, y_train)


# Predicting on the test set
y_pred = model.predict(X_test_tfidf)


# Results
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

y_proba = model.predict_proba(X_test_tfidf)[:, 1]  # Probability for the positive class

# Confusion matrix
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap="Blues")
plt.title("Matrice de confusion")
plt.show()

# Courbe ROC (binary)
fpr, tpr, thresholds = roc_curve(y_test, y_proba)
roc_auc = auc(fpr, tpr)

plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('Taux de faux positifs (FPR)')
plt.ylabel('Taux de vrais positifs (TPR)')
plt.title('Courbe ROC')
plt.legend(loc="lower right")
plt.grid(True)
plt.show()


# Test function to classify a phrase using the trained model and TF-IDF vectorizer
def test_phrase(model, tfidf, phrase):
    # Transform the phrase into a TF-IDF vector
    phrase_tfidf = tfidf.transform([phrase])

    # Predict the class (0 = Non-Hate Speech, 1 = Hate Speech)
    prediction = model.predict(phrase_tfidf)

    # Return the predicted class
    return "Hate Speech" if prediction[0] == 1 else "Non-Hate Speech"

# Example tests
example_1 = "Noana ah zany"
print(f"The phrase '{example_1}' is classified as: {test_phrase(model, tfidf, example_1)}")

example_2 = "Ry vahoaka malagasy, tiako ny tanindrazako"
print(f"The phrase '{example_2}' is classified as: {test_phrase(model, tfidf, example_2)}")

example_3 = "Matesa daholo, indrindra ilay mpialona ireny"
print(f"The phrase '{example_3}' is classified as: {test_phrase(model, tfidf, example_3)}")

example_4 = "Ratsy be pory tenisany amin'ireny akanjo mavomavo be ireny."
print(f"The phrase '{example_4}' is classified as: {test_phrase(model, tfidf, example_4)}")

example_5 = "Otran'ny alika nareo"
print(f"The phrase '{example_5}' is classified as: {test_phrase(model, tfidf, example_5)}")

example_6 = "kindindrenin'ity zany dia manahirana olona foana"
print(f"The phrase '{example_6}' is classified as: {test_phrase(model, tfidf, example_6)}")
