# -*- coding: utf-8 -*-
"""finetuning-mBERT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KhXWeFASXC5jTdo3qAlb8wS19Sfg54J7
"""

# Import necessary libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, roc_curve, auc
from sklearn.preprocessing import label_binarize

from transformers import BertTokenizer, BertForSequenceClassification, get_linear_schedule_with_warmup
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report
import wandb
from tqdm import tqdm
import random
import os
import re
import json
import argparse
import gdown

# Configure the random seed for reproducibility
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
torch.cuda.manual_seed_all(SEED)

# ======================================== #
#             Configuration                #
# ======================================== #
MAX_LEN = 512
BATCH_SIZE = 16
EPOCHS = 4
LEARNING_RATE = 2e-5
MODEL_NAME = 'bert-base-multilingual-cased'

# Configure PYTORCH_CUDA_ALLOC_CONF
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'

# --- Configuration WandB
WANDB_API_KEY = "00623049297b4d6a9b3febd02306cf1294021a68"
os.environ["WANDB_API_KEY"] = WANDB_API_KEY


# =============================== #
#           Utility Functions     #
# =============================== #

# Parse command line arguments for configuration
def parse_args():
    """Parse les arguments en ligne de commande"""
    parser = argparse.ArgumentParser(description='Configuration for hate speech detection')
    
    # Data arguments
    parser.add_argument('--train_file_id', type=str, help='Google Drive ID for the training data file')
    parser.add_argument('--test_file_id', type=str, help='Google Drive ID for the test data file')
        
    # Training arguments
    parser.add_argument('--batch_size', type=int, help='Batch size for training')
    parser.add_argument('--epochs', type=int, help='Number of training epochs')
    parser.add_argument('--learning_rate', type=float, help='Learning rate')
    
    # W&B arguments
    parser.add_argument('--wandb_project', type=str, help='Weights & Biases project name')
    parser.add_argument('--wandb_name', type=str, help='Weights & Biases run name')
    parser.add_argument('--wandb_key', type=str, help='Weights & Biases API key')
    
    return parser.parse_args()

# Clean text function to preprocess input text
def clean_text(texte):
    # clean special text
    mots_vides = ["voir plus", "lire la suite", "en savoir plus"]
    texte = texte.lower()
    for mot in mots_vides:
        texte = texte.replace(mot, "")

    # clean urls
    texte = re.sub(r'http\S+|www\S+|https\S+', '', texte)

    # clean numbers
    texte = re.sub(r'\d+', '', texte)

    return texte.strip()

# Plot confusion matrix using seaborn heatmap
def plot_confusion_matrix(labels, preds, class_names):
    cm = confusion_matrix(labels, preds)
    plt.figure(figsize=(8,6))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
                xticklabels=class_names,
                yticklabels=class_names)
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.title("Confusion Matrix")
    wandb.log({"confusion_matrix": wandb.Image(plt)})
    plt.close()

# Plot ROC curve and Precision-Recall curve
def plot_roc_curve(fpr, tpr, roc_auc, prefix=""):
    plt.figure(figsize=(8,6))
    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title(f'ROC Curve - {prefix}')
    plt.legend(loc="lower right")
    wandb.log({f"roc_curve_{prefix}": wandb.Image(plt)})
    plt.close()

# Load data from Google Drive using file ID
def load_data_by_id(file_id):
    """Charge les données à partir de Google Drive en utilisant l'ID de fichier"""
    local_file_name = f"downloaded_data_{file_id}.csv"
    try:
        print(f"Downloading file from {file_id}...")
        file_url = f'https://drive.google.com/uc?id={file_id}'
        gdown.download(file_url, local_file_name, quiet=False)
        file_path = local_file_name
        df = pd.read_csv(file_path)
        print("CSV file successfully downloaded and loaded.")
        return df
    except Exception as e:
        print(f"Error downloading/loading CSV file from URL: {e}")
        exit()

# load data 
def load_data(file_id):
    # Load data from Google Drive using the provided file ID
    df = load_data_by_id(file_id)

    # dropna
    df = df.dropna()

    # clean text
    df["text"] = df["text"].apply(clean_text)

    # cast label value to int
    df['label'] = df['label'].astype(float).astype('int')

    texts = df['text'].values
    labels = df['label'].values
    return texts, labels

# =========================== #
#           Dataset           #
# =========================== #
class HateSpeechDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_len):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = str(self.texts[idx])
        label = int(self.labels[idx])

        encoding = self.tokenizer.encode_plus(
            text,
            add_special_tokens=True,
            max_length=self.max_len,
            return_token_type_ids=False,
            padding='max_length',
            truncation=True,
            return_attention_mask=True,
            return_tensors='pt',
        )

        return {
            'text': text,
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'label': torch.tensor(label, dtype=torch.long)
        }

# Create DataLoader
def create_data_loader(X, y, tokenizer, max_len, batch_size, shuffle=True):
    dataset = HateSpeechDataset(
        texts=X,
        labels=y,
        tokenizer=tokenizer,
        max_len=max_len
    )
    return DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=shuffle
    )

# Evaluation function to compute loss and metrics
def eval_model(model, data_loader, device):
    model = model.eval()
    val_loss = 0
    correct_predictions = 0
    all_preds = []
    all_probs = []
    all_labels = []

    with torch.no_grad():
        print(f"length = {len(data_loader.dataset)}")
        for batch in data_loader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['label'].to(device)

            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                labels=labels
            )

            val_loss += outputs.loss.item()
            probs = torch.softmax(outputs.logits, dim=1)
            preds = torch.argmax(probs, dim=1)

            correct_predictions += (preds == labels).sum().item()
            all_preds.extend(preds.cpu().numpy())
            all_probs.extend(probs.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    avg_loss = val_loss / len(data_loader)
    accuracy = accuracy_score(all_labels, all_preds)
    f1 = f1_score(all_labels, all_preds)
    precision = precision_score(all_labels, all_preds)
    recall = recall_score(all_labels, all_preds)

    return avg_loss, accuracy, f1, precision, recall, all_labels, all_preds, np.array(all_probs)

# Fonction d'entraînement
def train_epoch(model, data_loader, optimizer, device, scheduler):
    model = model.train()
    losses = []
    correct_predictions = 0

    for d in tqdm(data_loader, desc="Training"):
        input_ids = d["input_ids"].to(device)
        attention_mask = d["attention_mask"].to(device)
        labels = d["label"].to(device)

        outputs = model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            labels=labels
        )

        loss = outputs.loss
        logits = outputs.logits

        preds = torch.argmax(logits, dim=1)
        correct_predictions += torch.sum(preds == labels)
        losses.append(loss.item())

        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()
        scheduler.step()
        optimizer.zero_grad()

    accuracy = correct_predictions.double() / len(data_loader.dataset)
    avg_loss = np.mean(losses)

    return avg_loss, accuracy

# =================================== #
#               Main                  #
# =================================== #

# Parse command line arguments
args = parse_args()

BATCH_SIZE = args.batch_size if args.batch_size else BATCH_SIZE
EPOCHS = args.epochs if args.epochs else EPOCHS
LEARNING_RATE = args.learning_rate if args.learning_rate else LEARNING_RATE

# Setup Weights & Biases (W&B) for experiment tracking
WANDB_API_KEY = args.wandb_key
os.environ["WANDB_API_KEY"] = WANDB_API_KEY

wandb.init(
    project=args.wandb_project,
    name=args.wandb_name,
    config={
        "batch_size": BATCH_SIZE,
        "epochs": EPOCHS,
        "lr": LEARNING_RATE,
        "model": MODEL_NAME
    }
)

"""## Configuration du modèle et des paramètres"""

# Initialisation du tokenizer et du modèle mBERT
tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)
model = BertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)

# Log hyperparameters to W&B
wandb.config.update({
    "max_len": MAX_LEN,
    "batch_size": BATCH_SIZE,
    "epochs": EPOCHS,
    "learning_rate": LEARNING_RATE,
    "model": MODEL_NAME,
    "seed": SEED
})

# loading the dataset
texts, labels = load_data(args.train_file_id)
X_test, y_test = load_data(args.test_file_id)
X_train, X_val, y_train, y_val = train_test_split(texts, labels, test_size=0.1, random_state=SEED, stratify=labels)

# Create DataLoaders for training, validation, and test sets
train_data_loader = create_data_loader(X_train, y_train, tokenizer, MAX_LEN, BATCH_SIZE)
val_data_loader = create_data_loader(X_val, y_val, tokenizer, MAX_LEN, BATCH_SIZE, shuffle=False)
test_data_loader = create_data_loader(X_test, y_test, tokenizer, MAX_LEN, BATCH_SIZE, shuffle=False)

# Configure the optimizer and learning rate scheduler
optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)
total_steps = len(train_data_loader) * EPOCHS
scheduler = get_linear_schedule_with_warmup(
    optimizer,
    num_warmup_steps=0,
    num_training_steps=total_steps
)

# set device for training
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)

# Training loop
best_accuracy = 0
for epoch in range(EPOCHS):
    print(f'Epoch {epoch + 1}/{EPOCHS}')
    print('-' * 10)

    train_loss, train_acc = train_epoch(
        model, train_data_loader, optimizer, device, scheduler
    )

    val_loss, val_acc, val_f1, val_precision, val_recall, _, _, _ = eval_model(
        model, val_data_loader, device
    )

    print(f'Train loss: {train_loss:.4f}, accuracy: {train_acc:.4f}')
    print(f'Val loss: {val_loss:.4f}, accuracy: {val_acc:.4f}')
    print(f'Val F1: {val_f1:.4f}, Precision: {val_precision:.4f}, Recall: {val_recall:.4f}')

    # log metrics to W&B
    wandb.log({
        "epoch": epoch,
        "train_loss": train_loss,
        "train_accuracy": train_acc,
        "val_loss": val_loss,
        "val_accuracy": val_acc,
        "val_f1": val_f1,
        "val_precision": val_precision,
        "val_recall": val_recall
    })

    # Save the model if validation accuracy improves
    if val_acc > best_accuracy:
        torch.save(model.state_dict(), 'best_model.bin')
        best_accuracy = val_acc
        print(f'New best model saved with accuracy: {best_accuracy:.4f}')

# Load the best model for evaluation
model.load_state_dict(torch.load('best_model.bin'))

# =================================== #
#           Evaluation                #
# =================================== #

# Evaluate the model on the test set
test_loss, test_acc, test_f1, test_precision, test_recall, test_labels, test_preds, test_probs = eval_model(
    model, test_data_loader, device
)

# Confusion matrix
plot_confusion_matrix(test_labels, test_preds, ["non-hate", "hate"])

# Courbe ROC
fpr, tpr, _ = roc_curve(test_labels, test_probs[:, 1])
roc_auc = auc(fpr, tpr)

# Plot ROC curve
plot_roc_curve(fpr, tpr, roc_auc)

print(f'Test loss: {test_loss:.4f}, accuracy: {test_acc:.4f}')
print(f'Test F1: {test_f1:.4f}, Precision: {test_precision:.4f}, Recall: {test_recall:.4f}')
print(classification_report(test_labels, test_preds, target_names=['non-hate', 'hate']))

# Log test metrics to W&B
wandb.log({
    "test_loss": test_loss,
    "test_accuracy": test_acc,
    "test_f1": test_f1,
    "test_recall": test_recall,
    "test_auc": roc_auc,
    "confusion_matrix": conf_mat.tolist(),
    "roc_curve": {
        "fpr": fpr.tolist(),
        "tpr": tpr.tolist(),
        "auc": roc_auc
    },
    "classification_report": classification_report(
        test_labels, test_preds, target_names=['non-hate', 'hate'], output_dict=True)
})

# Save the final model and tokenizer
torch.save(model.state_dict(), 'final_model.bin')
tokenizer.save_pretrained('./saved_tokenizer/')
wandb.save('best_model.bin')
wandb.save('final_model.bin')

# Finish the WandB run
wandb.finish()