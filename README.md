# ğŸ§  DÃ©tection de discours haineux en malgache

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

Ce projet a pour objectif la dÃ©tection automatique de discours haineux en langue malgache Ã  partir de donnÃ©es textuelles. Il utilise des modÃ¨les de traitement du langage naturel multilingues et prÃ©-entraÃ®nÃ©s, adaptÃ©s pour la classification binaire (haineux vs non-haineux).

---

## ğŸ” Objectifs

- Ã‰tudier lâ€™efficacitÃ© de **modÃ¨les multilingues prÃ©-entraÃ®nÃ©s** (mT5, mBERT) pour la classification de textes haineux en malgache.
- Construire un **corpus annotÃ©** de discours malgaches provenant de traductions et de donnÃ©es sociales rÃ©elles.
- Ã‰valuer diffÃ©rentes stratÃ©gies de fine-tuning et de gestion du dÃ©sÃ©quilibre des classes.

---

## ğŸ§¾ DonnÃ©es

Le corpus est composÃ© de deux sources :
1. âœ… **DonnÃ©es traduites** : Traduction en malgache de datasets publics (HateXplain, DGHD, slur-corpus, HateCheck, OLID, SWAD, Labeled_data, twitter_sexism, twitter_racism) via MadLab-7B-MT-BT.
2. ğŸ—£ï¸ **DonnÃ©es Facebook Malagasy** : ScrappÃ©es manuellement sur des publications ciblÃ©s, nettoyÃ©es et annotÃ©es manuellement.

Les donnÃ©es combinÃ©es comportent :
- ~25,000 exemples
- â‰ˆ32.5% de discours haineux

---

## ğŸ§  ModÃ¨les fine-tunÃ©s

### ğŸ”· mT5-small

- Encoder de mT5 avec **Mean Pooling**
- Classifieur 512 â†’ 256 â†’ 2
- Utilise la **Focal Loss** pour gÃ©rer le dÃ©sÃ©quilibre
- Apprentissage supervisÃ© sur donnÃ©es combinÃ©es

ğŸ“‚ Dossier : `mt5_finetuning/`

### ğŸ”¶ mBERT (bert-base-multilingual-cased)

- [CLS] token â†’ Classifieur 768 â†’ 256 â†’ 2
- Utilise la **CrossEntropyLoss standard**
- Fine-tuning classique sur les mÃªmes donnÃ©es

ğŸ“‚ Dossier : `mbert_finetuning/`

## ğŸ§ª Suivi des expÃ©riences

- Suivi complet avec **Weights & Biases (wandb)**
- Visualisations : matrices de confusion, courbes ROC et Precision-Recall
- Sauvegarde automatique des meilleurs modÃ¨les

---

