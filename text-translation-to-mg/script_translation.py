# -*- coding: utf-8 -*-
"""translate_to_mg.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1k_67S1GyHbaSfObRrw2e9Ns1wSq4ZHfC
"""

# -*- coding: utf-8 -*-
"""Hate Speech - TextTraslationToMg.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vzoa-oZphVH-qMU0D1LmP6OQreaP76f_

### Script translation text(en) to malagasy(mg)
"""

import subprocess
import sys
import os

def install_package(package_name):
    try:
        subprocess.check_call([sys.executable, "-m", "pip", "install", "-U", package_name])
        print(f"Package '{package_name}' installé ou mis à jour avec succès.")
        return True
    except subprocess.CalledProcessError as e:
        print(f"Erreur lors de l'installation du package '{package_name}': {e}")
        return False

# Installer tous les packages nécessaires
# install_package('tqdm')
# install_package('pandas')
install_package("bitsandbytes")
# install_package('transformers')
# install_package('torch')
# install_package('wandb')
# install_package('gdown')

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import pandas as pd
import torch
from tqdm.auto import tqdm
import wandb
import argparse
import gdown

# Définir les arguments pour le script
parser = argparse.ArgumentParser(description='Translate a portion of a CSV file using MADLAD-400-7B-MT-BT.')
parser.add_argument('--start_index', type=int, default=0, help='Index de la première ligne à traiter (inclusive).')
parser.add_argument('--end_index', type=int, default=10000, help='Index de la dernière ligne à traiter (exclusive).')
parser.add_argument('--batch_size', type=int, default=16, help='Taille du lot pour la traduction.')
parser.add_argument('--file_id', type=str, required=True, help='Id du fichier public sur drive')
args = parser.parse_args()

start_index = args.start_index
end_index = args.end_index
batch_size = args.batch_size
file_id = args.file_id

# Vérifier si un GPU est disponible et le définir comme device, sinon utiliser le CPU
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Utilisation de l'appareil : {device}")

# modèle MADLAD-400-7B-MT-BT avec quantification 8 bits
model_name = "google/madlad400-7b-mt-bt"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name, load_in_8bit=True, device_map='auto')

# Télécharger le fichier CSV depuis l'URL Google Drive
local_file_name = f"downloaded_data_{file_id}.csv"
try:
    print(f"Téléchargement du fichier depuis {file_id}...")
    file_url = f'https://drive.google.com/uc?id={file_id}'
    gdown.download(file_url, local_file_name, quiet=False)
    file_path = local_file_name
    df = pd.read_csv(file_path)
    print("Fichier CSV téléchargé et chargé avec succès.")
except Exception as e:
    print(f"Erreur lors du téléchargement ou du chargement du fichier CSV depuis l'URL: {e}")
    exit()

# Vérifier que la colonne 'text' existe
if 'text' not in df.columns:
    print("Erreur: La colonne 'text' n'existe pas dans le fichier CSV.")
    exit()

# Sélectionner la portion du DataFrame à traiter
df_subset = df[start_index:end_index]
print(f"Traitement des lignes de {start_index} à {end_index - 1} par lots de {batch_size}.")

def translate_batch(texts):
    input_texts = [f"<2mg> {text}" for text in texts]
    inputs = tokenizer(input_texts, return_tensors="pt", padding=True, truncation=True).to(device)
    with torch.no_grad():
        generated_tokens = model.generate(
            **inputs,
            max_length=512
        )
    translated_texts = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)
    return translated_texts

translations = [''] * len(df_subset)

# --- Configuration WandB
WANDB_API_KEY = "00623049297b4d6a9b3febd02306cf1294021a68"
os.environ["WANDB_API_KEY"] = WANDB_API_KEY # Définir la variable d'environnement pour WandB

# Initialiser WandB
wandb.init(project="hate-speech-translated-malagasy", name=f"translation-run-{start_index}-{end_index}")
# --- Fin de la configuration WandB ---


total_batches = len(df_subset) // batch_size + (1 if len(df_subset) % batch_size != 0 else 0)

# Appliquer la traduction par lots avec une barre de progression et suivi WandB
for i in tqdm(range(0, len(df_subset), batch_size), desc=f"Traduction ({start_index}-{end_index})"):
    batch = df_subset['text'][i:i + batch_size].tolist()
    translated_batch = translate_batch(batch)
    translations[i:i + batch_size] = translated_batch

    batch_number = i // batch_size + 1
    wandb.log({"progress": batch_number / total_batches * 100,
               "lines_processed": i + len(batch)})

# Ajouter la liste des traductions comme une nouvelle colonne au sous-ensemble
df_subset['translated'] = translations

# Créer un tableau WandB à partir du sous-ensemble traduit (contenant 'text', 'label' et 'translated')
wandb_table = wandb.Table(data=df_subset[['text', 'label', 'translated']], columns=["text", "label", "translated"])

# Enregistrer le tableau dans WandB
wandb.log({"translations_segment": wandb_table})

# Fermer la run WandB
wandb.finish()