# -*- coding: utf-8 -*-
"""finetuning-mT5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17Ke-jQYifphT3zDkmS5xG6XoLaPJJZ6P
"""

from google.colab import drive
drive.mount('/content/drive')

# Import necessary libraries
import torch
import wandb
import numpy as np
import pandas as pd
from torch import nn, optim
import torch.nn.functional as F
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer, MT5Model, Adafactor
from sklearn.model_selection import train_test_split
from sklearn.metrics import (accuracy_score, f1_score,
                           confusion_matrix, roc_auc_score,
                           classification_report, roc_curve, auc, precision_recall_curve)
import matplotlib.pyplot as plt
import seaborn as sns

from tqdm import tqdm
import os
import re

# --- Configuration WandB
WANDB_API_KEY = "00623049297b4d6a9b3febd02306cf1294021a68"
os.environ["WANDB_API_KEY"] = WANDB_API_KEY

# =================================================== #
#             Configuration and Focal Loss            #
# =================================================== #
class Config:
    # Data
    data_path = "/content/drive/MyDrive/hate_speech/mg/combined/combined_dataset_train.csv"
    data_test_path = "/content/drive/MyDrive/hate_speech/mg/combined/combined_dataset_test.csv"
    text_col = "text"
    label_col = "label"
    train_ratio = 0.8
    val_ratio = 0.1
    test_ratio = 0.1

    # Model
    model_name = "google/mt5-small"
    max_length = 512
    dropout_rate = 0.4

    # Training
    batch_size = 8
    epochs = 10
    learning_rate = 2e-5
    weight_decay = 0.05
    patience = 3
    optim_mode = 'min'
    optim_factor = 0.5
    early_stopping_patience = 10

    # W&B
    wandb_project = "hate_speech_malagasy_fv"
    wandb_name = "finetuning_mT5_combined"


"""
Focal Loss implementation - Used as-is from:
https://github.com/itakurah/Focal-loss-PyTorch

This implementation supports:
- Binary classification
- Multi-class classification
- Multi-label classification

Original paper: "Focal Loss for Dense Object Detection" (Lin et al., 2017)
"""
class FocalLoss(nn.Module):
    def __init__(self, gamma=2, alpha=None, reduction='mean', task_type='binary', num_classes=None):
        """
        Unified Focal Loss class for binary, multi-class, and multi-label classification tasks.
        :param gamma: Focusing parameter, controls the strength of the modulating factor (1 - p_t)^gamma
        :param alpha: Balancing factor, can be a scalar or a tensor for class-wise weights. If None, no class balancing is used.
        :param reduction: Specifies the reduction method: 'none' | 'mean' | 'sum'
        :param task_type: Specifies the type of task: 'binary', 'multi-class', or 'multi-label'
        :param num_classes: Number of classes (only required for multi-class classification)
        """
        super(FocalLoss, self).__init__()
        self.gamma = gamma
        self.alpha = alpha
        self.reduction = reduction
        self.task_type = task_type
        self.num_classes = num_classes

        # Handle alpha for class balancing in multi-class tasks
        if task_type == 'multi-class' and alpha is not None and isinstance(alpha, (list, torch.Tensor)):
            assert num_classes is not None, "num_classes must be specified for multi-class classification"
            if isinstance(alpha, list):
                self.alpha = torch.Tensor(alpha)
            else:
                self.alpha = alpha

    def forward(self, inputs, targets):
        """
        Forward pass to compute the Focal Loss based on the specified task type.
        :param inputs: Predictions (logits) from the model.
                       Shape:
                         - binary/multi-label: (batch_size, num_classes)
                         - multi-class: (batch_size, num_classes)
        :param targets: Ground truth labels.
                        Shape:
                         - binary: (batch_size,)
                         - multi-label: (batch_size, num_classes)
                         - multi-class: (batch_size,)
        """
        if self.task_type == 'binary':
            return self.binary_focal_loss(inputs, targets)
        elif self.task_type == 'multi-class':
            return self.multi_class_focal_loss(inputs, targets)
        elif self.task_type == 'multi-label':
            return self.multi_label_focal_loss(inputs, targets)
        else:
            raise ValueError(
                f"Unsupported task_type '{self.task_type}'. Use 'binary', 'multi-class', or 'multi-label'.")

    def binary_focal_loss(self, inputs, targets):
        """ Focal loss for binary classification. """
        probs = torch.sigmoid(inputs)
        targets = targets.float()

        # Compute binary cross entropy
        bce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')

        # Compute focal weight
        p_t = probs * targets + (1 - probs) * (1 - targets)
        focal_weight = (1 - p_t) ** self.gamma

        # Apply alpha if provided
        if self.alpha is not None:
            alpha_t = self.alpha * targets + (1 - self.alpha) * (1 - targets)
            bce_loss = alpha_t * bce_loss

        # Apply focal loss weighting
        loss = focal_weight * bce_loss

        if self.reduction == 'mean':
            return loss.mean()
        elif self.reduction == 'sum':
            return loss.sum()
        return loss

    def multi_class_focal_loss(self, inputs, targets):
        """ Focal loss for multi-class classification. """
        if self.alpha is not None:
            alpha = self.alpha.to(inputs.device)

        # Convert logits to probabilities with softmax
        probs = F.softmax(inputs, dim=1)

        # One-hot encode the targets
        targets_one_hot = F.one_hot(targets, num_classes=self.num_classes).float()

        # Compute cross-entropy for each class
        ce_loss = -targets_one_hot * torch.log(probs)

        # Compute focal weight
        p_t = torch.sum(probs * targets_one_hot, dim=1)  # p_t for each sample
        focal_weight = (1 - p_t) ** self.gamma

        # Apply alpha if provided (per-class weighting)
        if self.alpha is not None:
            alpha_t = alpha.gather(0, targets)
            ce_loss = alpha_t.unsqueeze(1) * ce_loss

        # Apply focal loss weight
        loss = focal_weight.unsqueeze(1) * ce_loss

        if self.reduction == 'mean':
            return loss.mean()
        elif self.reduction == 'sum':
            return loss.sum()
        return loss

    def multi_label_focal_loss(self, inputs, targets):
        """ Focal loss for multi-label classification. """
        probs = torch.sigmoid(inputs)

        # Compute binary cross entropy
        bce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')

        # Compute focal weight
        p_t = probs * targets + (1 - probs) * (1 - targets)
        focal_weight = (1 - p_t) ** self.gamma

        # Apply alpha if provided
        if self.alpha is not None:
            alpha_t = self.alpha * targets + (1 - self.alpha) * (1 - targets)
            bce_loss = alpha_t * bce_loss

        # Apply focal loss weight
        loss = focal_weight * bce_loss

        if self.reduction == 'mean':
            return loss.mean()
        elif self.reduction == 'sum':
            return loss.sum()
        return loss

# =========================== #
#           Model             #
# =========================== #
class MT5ForClassification(nn.Module):
     """
    Custom mT5 model for hate speech classification with Focal Loss.
    
    This class implements a sequence classification architecture based on mT5's encoder
    with a custom classifier head. It uses mean pooling over the token embeddings
    and a multi-layer perceptron for classification.
    
    Attributes:
        mt5 (MT5Model): The mT5 encoder model
        dropout (nn.Dropout): Dropout layer for regularization
        loss_fct (FocalLoss): Loss function for imbalanced classification
        classifier (nn.Sequential): Custom classifier head
        
    Args:
        config (Config): Configuration object containing model parameters
    
    Note:
        Uses unmodified Focal Loss implementation from:
        https://github.com/itakurah/Focal-loss-PyTorch
    """
    def __init__(self, config):
        super().__init__()
        self.mt5 = MT5Model.from_pretrained(config.model_name)
        self.dropout = nn.Dropout(config.dropout_rate)

        self.loss_fct = FocalLoss(gamma=2, alpha=0.9, task_type='binary')

        # Classifier with intermediate layer
        self.classifier = nn.Sequential(
            nn.Linear(self.mt5.config.d_model, 512),        # Input dim = mT5 hidden size
            nn.LayerNorm(512),                              # Normalization
            nn.GELU(),                                      # GELU activation
            nn.Dropout(config.dropout_rate),                # Regularization
            nn.Linear(512, 256),                            # Hidden layer
            nn.LayerNorm(256),
            nn.GELU(),
            nn.Dropout(config.dropout_rate),
            nn.Linear(256, 2)                               # Output layer (binary)
          )

    def forward(self, input_ids, attention_mask, labels=None):
        outputs = self.mt5.encoder(
            input_ids=input_ids,
            attention_mask=attention_mask
        )
        hidden_states = outputs.last_hidden_state

         # Mean Pooling - aggregate token embeddings
        mask_expanded = attention_mask.unsqueeze(-1).expand(hidden_states.size()).float()
        sum_embeddings = torch.sum(hidden_states * mask_expanded, dim=1)
        sum_mask = torch.clamp(mask_expanded.sum(1), min=1e-9)
        pooled = sum_embeddings / sum_mask

        pooled = self.dropout(pooled)
        logits = self.classifier(pooled)

        loss = None
        if labels is not None:
            labels_one_hot = F.one_hot(labels, num_classes=2).float()
            loss = self.loss_fct(logits, labels_one_hot)

        return logits, loss

# =========================== #
#           Dataset           #
# =========================== #
class HateSpeechDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_len):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        encoding = self.tokenizer(
            str(self.texts[idx]),
            max_length=self.max_len,
            padding="max_length",
            truncation=True,
            return_tensors="pt"
        )
        return {
            "input_ids": encoding["input_ids"].flatten(),
            "attention_mask": encoding["attention_mask"].flatten(),
            "labels": torch.tensor(self.labels[idx], dtype=torch.long)
        }

# =============================== #
#           Utility Functions     #
# =============================== #

# Setup Weights & Biases (W&B) for experiment tracking
def setup_wandb(config):
    wandb.init(
        project=config.wandb_project,
        name=config.wandb_name,
        config={
            "batch_size": config.batch_size,
            "epochs": config.epochs,
            "lr": config.learning_rate,
            "model": config.model_name,
            "split": f"{int(config.train_ratio*100)}/{int(config.val_ratio*100)}/{int(config.test_ratio*100)}"
        }
    )

# Plot confusion matrix using seaborn heatmap
def plot_confusion_matrix(labels, preds, class_names):
    cm = confusion_matrix(labels, preds)
    plt.figure(figsize=(8,6))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
                xticklabels=class_names,
                yticklabels=class_names)
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.title("Confusion Matrix")
    wandb.log({"confusion_matrix": wandb.Image(plt)})
    plt.close()

# Plot ROC curve and Precision-Recall curve
def plot_roc_curve(fpr, tpr, roc_auc, prefix=""):
    plt.figure(figsize=(8,6))
    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title(f'ROC Curve - {prefix}')
    plt.legend(loc="lower right")
    wandb.log({f"roc_curve_{prefix}": wandb.Image(plt)})
    plt.close()

# Plot Precision-Recall curve
def plot_precision_recall_curve(precision, recall, prefix=""):
    plt.figure(figsize=(8,6))
    plt.plot(recall, precision, color='blue', lw=2)
    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.title(f'Precision-Recall Curve - {prefix}')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    wandb.log({f"precision_recall_curve_{prefix}": wandb.Image(plt)})
    plt.close()

# Log metrics to W&B
def log_metrics(metrics, prefix=""):
    wandb.log({
        f"{prefix}accuracy": metrics["accuracy"],
        f"{prefix}f1": metrics["f1"],
        f"{prefix}roc_auc": metrics["roc_auc"],
        f"{prefix}loss": metrics["loss"],
        f"{prefix}report": metrics["report"]
    })

# Clean text function to preprocess input text
def clean_text(texte):
    mots_vides = ["voir plus", "lire la suite", "en savoir plus"]
    texte = texte.lower()
    for mot in mots_vides:
        texte = texte.replace(mot, "")

    # Suppression des URL
    texte = re.sub(r'http\S+|www\S+|https\S+', '', texte)

    # Suppression des nombres
    texte = re.sub(r'\d+', '', texte)

    return texte.strip()

# ========================== #
#         Main Function      #
# ========================== #
# Main function to run the training and evaluation pipeline
def main():
    config = Config()
    setup_wandb(config)

    # load data 
    df = pd.read_csv(config.data_path)
    df_test = pd.read_csv(config.data_test_path)

    # dropna
    df = df.dropna()
    df_test = df_test.dropna()

    # clean text
    df["text"] = df["text"].apply(clean_text)
    df_test["text"] = df_test["text"].apply(clean_text)

    # cast label value to int
    df['label'] = df['label'].astype(float).astype('int')
    df_test["label"] = df_test["label"].astype(float).astype("int")

    # Reset the index of df_test after dropping NaNs
    df_test = df_test.reset_index(drop=True)

    texts = df[config.text_col].values
    labels = df[config.label_col].values

    # Separate train, validation sets 90/10
    # Note: test set is already provided separately
    X_train, X_val, y_train, y_val = train_test_split(
        texts, labels,
        test_size=(config.val_ratio),
        stratify=labels,
        random_state=42
    )

    # Initialise tokenizer
    tokenizer = AutoTokenizer.from_pretrained(config.model_name)

    # tokenisation and dataset preparation
    # Note: max_length is set to 512 for mT5
    train_dataset = HateSpeechDataset(X_train, y_train, tokenizer, config.max_length)
    val_dataset = HateSpeechDataset(X_val, y_val, tokenizer, config.max_length)
    test_dataset = HateSpeechDataset(df_test[config.text_col], df_test[config.label_col], tokenizer, config.max_length)

    # DataLoader preparation
    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=config.batch_size)
    test_loader = DataLoader(test_dataset, batch_size=config.batch_size)

    # Initialize model, optimizer, and scheduler
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = MT5ForClassification(config).to(device)
    optimizer = optim.AdamW(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=config.optim_mode, factor=config.optim_factor, patience=config.patience, verbose=True)

    # Training loop
    best_val_loss = float("inf")
    epochs_no_improve = 0

    for epoch in range(config.epochs):
        # Training phase
        model.train()
        train_loss = 0
        all_preds = []
        all_labels = []

        for batch in tqdm(train_loader, desc=f"Epoch {epoch+1}/{config.epochs}"):
            inputs = {k: v.to(device) for k, v in batch.items()}

            optimizer.zero_grad()
            logits, loss = model(**inputs)
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            preds = torch.argmax(logits, dim=1)
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(inputs["labels"].cpu().numpy())

        # Validation phase
        model.eval()
        val_loss = 0
        val_preds = []
        val_labels = []
        val_probs = []

        with torch.no_grad():
            for batch in val_loader:
                inputs = {k: v.to(device) for k, v in batch.items()}
                logits, loss = model(**inputs)

                val_loss += loss.item()
                probs = torch.softmax(logits, dim=1)
                preds = torch.argmax(logits, dim=1)

                val_preds.extend(preds.cpu().numpy())
                val_labels.extend(inputs["labels"].cpu().numpy())
                val_probs.extend(probs[:, 1].cpu().numpy())

        fpr, tpr, _ = roc_curve(val_labels, val_probs)
        roc_auc = auc(fpr, tpr)
        precision, recall, _ = precision_recall_curve(val_labels, val_probs)

        # Calcul des métriques
        train_metrics = {
            "loss": train_loss / len(train_loader),
            "accuracy": accuracy_score(all_labels, all_preds),
            "f1": f1_score(all_labels, all_preds),
            "roc_auc": roc_auc_score(all_labels, all_preds),
            "report": classification_report(all_labels, all_preds, output_dict=True)
        }

        val_metrics = {
            "loss": val_loss / len(val_loader),
            "accuracy": accuracy_score(val_labels, val_preds),
            "f1": f1_score(val_labels, val_preds),
            "roc_auc": roc_auc_score(val_labels, val_probs),
            "fpr": fpr,
            "tpr": tpr,
            "precision": precision,
            "recall": recall,
            "report": classification_report(val_labels, val_preds, output_dict=True)
        }

        # Log W&B
        log_metrics(train_metrics, "train/")
        log_metrics(val_metrics, "val/")
        plot_confusion_matrix(val_labels, val_preds, ["non-hate", "hate"])
        plot_roc_curve(val_metrics['fpr'], val_metrics['tpr'], val_metrics['roc_auc'], "val")
        plot_precision_recall_curve(val_metrics['precision'], val_metrics['recall'], "val")

        # Early stopping
        if val_metrics["loss"] < best_val_loss:
            best_val_loss = val_metrics["loss"]
            epochs_no_improve = 0
            torch.save(model.state_dict(), "best_model.pt")
        else:
            epochs_no_improve += 1
            if epochs_no_improve >= config.early_stopping_patience:
                print(f"Early stopping at epoch {epoch+1}")
                break

        scheduler.step(val_metrics["loss"])

    # Evaluation on test set
    model.load_state_dict(torch.load("best_model.pt"))
    model.eval()

    test_loss = 0
    test_preds = []
    test_labels = []
    test_probs = []
    test_preds_prob = []
    test_true_labels = []

    with torch.no_grad():
        for batch in test_loader:
            inputs = {k: v.to(device) for k, v in batch.items()}
            logits, loss = model(**inputs)

            test_loss += loss.item()
            probs = torch.softmax(logits, dim=1)
            preds = torch.argmax(logits, dim=1)

            test_preds.extend(preds.cpu().numpy())
            test_labels.extend(inputs["labels"].cpu().numpy())
            test_probs.extend(probs[:, 1].cpu().numpy())

    test_preds_prob = np.array(test_probs)
    test_true_labels = np.array(test_labels)
    final_test_predictions_binary = (test_preds_prob >= 0.5).astype(int)
    
    test_metrics = {
        "loss": test_loss / len(test_loader),
        "accuracy": accuracy_score(test_true_labels, final_test_predictions_binary),
        "f1": f1_score(test_true_labels, final_test_predictions_binary, average='binary', pos_label=1),
        "roc_auc": roc_auc_score(test_true_labels, test_preds_prob),
        "report": classification_report(test_labels, test_preds, output_dict=True)
    }

    log_metrics(test_metrics, "test/")
    plot_confusion_matrix(test_labels, test_preds, ["non-hate", "hate"])

    # Sauvegarde finale
    tokenizer.save_pretrained("model/")
    torch.save(model.state_dict(), "model/final_model.pt")
    wandb.save("model/*")

    print("\n=== Résultats finaux ===")
    print(f"Test Accuracy: {test_metrics['accuracy']:.4f}")
    print(f"Test F1: {test_metrics['f1']:.4f}")
    print(f"Test ROC-AUC: {test_metrics['roc_auc']:.4f}")

    wandb.finish()
    return model, tokenizer

def predict_text(model, tokenizer, text, device):
    model.eval()
    encoding = tokenizer(
        text,
        padding=True,
        truncation=True,
        return_tensors='pt',
        max_length=128
    ).to(device)

    with torch.no_grad():
        outputs, _ = model(
            input_ids=encoding['input_ids'],
            attention_mask=encoding['attention_mask']
        )
        prediction = torch.argmax(outputs, dim=1)

    if prediction.item() == 0:
        print(text, " => Pas de haine 👍")
    else:
        print(text, " => Discours haineux 🚨")

# Main entry point for the script
# This will run the training and evaluation pipeline
if __name__ == "__main__":
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model, tokenizer = main()

    # example usage of prediction
    predict_text(model, tokenizer, "Olona mainty be daholo nareo zany", device)
    predict_text(model, tokenizer, "Tiako be daholo nareo", device)
    predict_text(model, tokenizer, "Be fitiavana be io olona io", device)
    predict_text(model, tokenizer, "Aza be fanotaniana fa ataovy fotsiny", device)
    predict_text(model, tokenizer, "Solution numero 13", device)
    predict_text(model, tokenizer, "Tonga dia vonoy any", device)
    predict_text(model, tokenizer, "Veloma daholo", device)
    predict_text(model, tokenizer, "Veloma pory daholo", device)
    predict_text(model, tokenizer, "Tiako be daholo nareo", device)
    predict_text(model, tokenizer, "Tiako be daholo nareo lelena", device)
    predict_text(model, tokenizer, "Mahafinaritra kely ianao", device)
    predict_text(model, tokenizer, "Tena tsy tiako mitsy ny ahy ilay olona mainty be otran'zao ny tafita", device)
    predict_text(model, tokenizer, "Jereo anie ity tongony tapesaka be ity e", device)
    predict_text(model, tokenizer, "Omeo an ny vahoaka izy", device)
    predict_text(model, tokenizer, "Tsy tiako mihitsy ny olona mainty be", device)